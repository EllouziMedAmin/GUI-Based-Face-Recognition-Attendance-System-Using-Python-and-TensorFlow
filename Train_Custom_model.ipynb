{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMZDGI3RYaw4m1eYpmJRKqo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EllouziMedAmin/GUI-Based-Face-Recognition-Attendance-System-Using-Python-and-TensorFlow/blob/main/Train_Custom_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create_unknown_dataset.py\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "from sklearn.datasets import fetch_lfw_people\n",
        "\n",
        "# 1. Setup Folder\n",
        "save_dir = \"dataset/unknown\"\n",
        "if not os.path.exists(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "\n",
        "# 2. Download Data (this might take a minute the first time)\n",
        "print(\"Downloading Labeled Faces in the Wild (LFW) dataset...\")\n",
        "lfw_people = fetch_lfw_people(min_faces_per_person=1, resize=None) # Get original images\n",
        "\n",
        "# 3. Save 50 Random Faces\n",
        "print(f\"Dataset loaded. Total faces available: {len(lfw_people.images)}\")\n",
        "indices = np.random.choice(len(lfw_people.images), 50, replace=False)\n",
        "\n",
        "count = 0\n",
        "for i in indices:\n",
        "    # LFW images are in 0-1 float range, convert to 0-255 for saving\n",
        "    face_img = (lfw_people.images[i] * 255).astype(np.uint8)\n",
        "\n",
        "    # Resize to match your webcam crop (224x224)\n",
        "    face_img = cv2.resize(face_img, (224, 224))\n",
        "\n",
        "    # Save\n",
        "    file_path = os.path.join(save_dir, f\"unknown_{count}.jpg\")\n",
        "    cv2.imwrite(file_path, face_img)\n",
        "    count += 1\n",
        "\n",
        "print(f\"Successfully saved {count} images to {save_dir}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fqdizuvf6FDJ",
        "outputId": "77058123-3c30-4b1f-9bd7-8142453ddec4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading Labeled Faces in the Wild (LFW) dataset...\n",
            "Dataset loaded. Total faces available: 13233\n",
            "Successfully saved 50 images to dataset/unknown\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pkd4fTHc77O0",
        "outputId": "3dc76f04-f7ec-45cd-d1f2-2476024b1b63"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/dataset/Mehdi"
      ],
      "metadata": {
        "id": "QC2oMKDt9zrB"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rsync -av /content/drive/MyDrive/Mehdi/ /content/dataset/Mehdi/\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2lSmrw48_eK",
        "outputId": "ba01a485-f746-43f0-81ac-c07c510a5cc7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sending incremental file list\n",
            "./\n",
            "IMG-20251214-WA0001.jpg\n",
            "IMG-20251214-WA0003.jpg\n",
            "IMG-20251214-WA0004.jpg\n",
            "IMG-20251214-WA0005.jpg\n",
            "IMG-20251214-WA0006.jpg\n",
            "IMG-20251214-WA0007.jpg\n",
            "IMG-20251214-WA0008.jpg\n",
            "IMG-20251214-WA0009.jpg\n",
            "IMG-20251214-WA0010.jpg\n",
            "IMG-20251214-WA0011.jpg\n",
            "IMG-20251214-WA0012.jpg\n",
            "IMG-20251214-WA0013.jpg\n",
            "IMG-20251214-WA0014.jpg\n",
            "IMG-20251214-WA0015.jpg\n",
            "IMG-20251214-WA0016.jpg\n",
            "IMG-20251214-WA0017.jpg\n",
            "IMG-20251214-WA0018.jpg\n",
            "IMG-20251214-WA0019.jpg\n",
            "IMG-20251214-WA0020.jpg\n",
            "IMG-20251214-WA0021.jpg\n",
            "IMG-20251214-WA0022.jpg\n",
            "IMG-20251214-WA0023.jpg\n",
            "IMG-20251214-WA0024.jpg\n",
            "IMG-20251214-WA0025.jpg\n",
            "WhatsApp Image 2025-12-14 à 15.47.05_59da9b47.jpg\n",
            "WhatsApp Image 2025-12-14 à 15.59.54_61ab9ab1.jpg\n",
            "WhatsApp Image 2025-12-14 à 15.59.54_8134665a.jpg\n",
            "WhatsApp Image 2025-12-14 à 15.59.54_f9278437.jpg\n",
            "WhatsApp Image 2025-12-14 à 15.59.56_03c91f0f.jpg\n",
            "WhatsApp Image 2025-12-14 à 15.59.56_2568a9c3.jpg\n",
            "WhatsApp Image 2025-12-14 à 15.59.56_524fca3f.jpg\n",
            "WhatsApp Image 2025-12-14 à 15.59.56_647d867c.jpg\n",
            "WhatsApp Image 2025-12-14 à 15.59.56_7b5cb81e.jpg\n",
            "WhatsApp Image 2025-12-14 à 15.59.56_9632857f.jpg\n",
            "WhatsApp Image 2025-12-14 à 15.59.56_a5189e4c.jpg\n",
            "WhatsApp Image 2025-12-14 à 15.59.56_c85b1006.jpg\n",
            "WhatsApp Image 2025-12-14 à 15.59.56_f7892a50.jpg\n",
            "WhatsApp Image 2025-12-14 à 15.59.57_179cf01a.jpg\n",
            "WhatsApp Image 2025-12-14 à 15.59.57_181512a2.jpg\n",
            "WhatsApp Image 2025-12-14 à 15.59.57_2984806d.jpg\n",
            "WhatsApp Image 2025-12-14 à 15.59.57_2c49c363.jpg\n",
            "WhatsApp Image 2025-12-14 à 15.59.57_3b1dded5.jpg\n",
            "WhatsApp Image 2025-12-14 à 15.59.57_4d85f2c4.jpg\n",
            "WhatsApp Image 2025-12-14 à 15.59.57_5727c404.jpg\n",
            "WhatsApp Image 2025-12-14 à 15.59.57_58e02385.jpg\n",
            "WhatsApp Image 2025-12-14 à 15.59.57_721038ea.jpg\n",
            "WhatsApp Image 2025-12-14 à 15.59.57_754e8a2d.jpg\n",
            "WhatsApp Image 2025-12-14 à 15.59.57_78a4c49f.jpg\n",
            "WhatsApp Image 2025-12-14 à 15.59.57_908d67c1.jpg\n",
            "WhatsApp Image 2025-12-14 à 15.59.57_978d3f8f.jpg\n",
            "WhatsApp Image 2025-12-14 à 15.59.57_9b9147ea.jpg\n",
            "WhatsApp Image 2025-12-14 à 15.59.57_aa19fbaf.jpg\n",
            "WhatsApp Image 2025-12-14 à 15.59.57_b3a1b7ea.jpg\n",
            "WhatsApp Image 2025-12-14 à 15.59.57_bc49d356.jpg\n",
            "WhatsApp Image 2025-12-14 à 15.59.57_bf5fe8f1.jpg\n",
            "WhatsApp Image 2025-12-14 à 15.59.57_da7b7cf8.jpg\n",
            "WhatsApp Image 2025-12-14 à 15.59.57_dd5d638a.jpg\n",
            "WhatsApp Image 2025-12-14 à 15.59.57_de897a38.jpg\n",
            "WhatsApp Image 2025-12-14 à 15.59.57_ea419835.jpg\n",
            "WhatsApp Image 2025-12-14 à 15.59.57_faf2f893.jpg\n",
            "\n",
            "sent 5,822,208 bytes  received 1,159 bytes  247,802.85 bytes/sec\n",
            "total size is 5,817,260  speedup is 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "s4__klxW3W6t"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
        "from tensorflow.keras.models import Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Setup Data Generators (Data Augmentation adds \"Effort\" points)\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    'dataset/',\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='binary', # Mehdi vs Unknown\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "validation_generator = train_datagen.flow_from_directory(\n",
        "    'dataset/',\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='binary',\n",
        "    subset='validation'\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "bJ_NLiA85c7B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "523ec524-dd6c-4cfc-f967-88fb297c284c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 88 images belonging to 2 classes.\n",
            "Found 22 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_generator.class_indices"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r3NB37eC_A8-",
        "outputId": "211f3868-27ea-4653-8a1d-255df7437add"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Amin': 0, 'unknown': 1}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Build the Model (Transfer Learning)\n",
        "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "base_model.trainable = False # Freeze base model\n"
      ],
      "metadata": {
        "id": "CKAdHN-75gPY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40475ab8-83d8-415c-ead2-1bc6288e74b0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "x = Dropout(0.5)(x) # Prevents overfitting\n",
        "predictions = Dense(1, activation='sigmoid')(x) # Binary output (0 or 1)\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n"
      ],
      "metadata": {
        "id": "5v1sUw4g5mGh"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Train\n",
        "print(\"Starting training...\")\n",
        "model.fit(train_generator, epochs=5, validation_data=validation_generator)\n",
        "# 4. Save\n",
        "model.save('attendance_model_mehdi_only.h5')\n",
        "print(\"Model saved as attendance_model.h5\")\n"
      ],
      "metadata": {
        "id": "-ZHJadrJ5ojb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23211600-9d8a-4784-8076-969c94df8e36"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 11s/step - accuracy: 0.5110 - loss: 0.8410 - val_accuracy: 1.0000 - val_loss: 0.1852\n",
            "Epoch 2/5\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 732ms/step - accuracy: 0.9556 - loss: 0.2181 - val_accuracy: 1.0000 - val_loss: 0.0663\n",
            "Epoch 3/5\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 427ms/step - accuracy: 0.9584 - loss: 0.1087 - val_accuracy: 1.0000 - val_loss: 0.0178\n",
            "Epoch 4/5\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 431ms/step - accuracy: 1.0000 - loss: 0.0589 - val_accuracy: 1.0000 - val_loss: 0.0280\n",
            "Epoch 5/5\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 431ms/step - accuracy: 1.0000 - loss: 0.0331 - val_accuracy: 1.0000 - val_loss: 0.0045\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved as attendance_model.h5\n"
          ]
        }
      ]
    }
  ]
}